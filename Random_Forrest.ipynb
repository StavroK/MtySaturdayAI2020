{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forrest.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJ1aTf2cNsfYmQU+ZIoG0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StavroK/MtySaturdayAI2020/blob/master/Random_Forrest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkkN9-pKrAWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant packages and modules\n",
        "from csv import DictReader\n",
        "from csv import DictWriter\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.layers import  Input, Embedding, Dot, Reshape, Dense\n",
        "from tensorflow.python.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvv0_OMdrMkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise global variables\n",
        "label_ref = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
        "label_ref_rev = {0: 'agree', 1: 'disagree', 2: 'discuss', 3: 'unrelated'}\n",
        "stop_words = [\n",
        "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
        "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
        "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
        "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
        "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
        "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
        "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
        "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
        "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
        "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
        "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
        "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
        "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
        "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
        "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
        "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
        "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
        "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
        "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
        "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
        "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
        "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
        "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
        "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
        "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjo4pCpdq_B2",
        "colab_type": "text"
      },
      "source": [
        "Define class for Example data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzYtqbHqsgHm",
        "colab_type": "text"
      },
      "source": [
        "Read Example data from CSV file.\n",
        "---\n",
        "            - **INPUTS**  -> filename: str, filename + extension\n",
        "            - **OUTPUTS** -> rows: list, of dict per instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYFDKUcQrRyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define data class\n",
        "class ModelData:\n",
        "\n",
        "    def __init__(self, file_instances, file_bodies):\n",
        "\n",
        "        # Load data\n",
        "        self.instances = self.read(file_instances)\n",
        "        bodies = self.read(file_bodies)\n",
        "        self.heads = {}\n",
        "        self.bodies = {}\n",
        "\n",
        "        # Process instances\n",
        "        for instance in self.instances:\n",
        "            if instance['Headline'] not in self.heads:\n",
        "                head_id = len(self.heads)\n",
        "                self.heads[instance['Headline']] = head_id\n",
        "            instance['Body ID'] = int(instance['Body ID'])\n",
        "\n",
        "        # Process bodies\n",
        "        for body in bodies:\n",
        "            self.bodies[int(body['Body ID'])] = body['articleBody']\n",
        "\n",
        "    def read(self, filename):\n",
        "\n",
        "\n",
        "\n",
        "        # Initialise\n",
        "        rows = []\n",
        "\n",
        "        # Process file\n",
        "        with open(filename, \"r\", encoding='utf-8') as table:\n",
        "            r = DictReader(table)\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "\n",
        "        return rows\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7sekX1dxgcL",
        "colab_type": "text"
      },
      "source": [
        "    \"\"\"\n",
        "    Process train set, create relevant vectorizers\n",
        "    Args:\n",
        "        train: ModelData object, train set\n",
        "        test: ModelData object, test set\n",
        "        lim_unigram: int, number of most frequent words to consider\n",
        "    Returns:\n",
        "        train_set: list, of numpy arrays\n",
        "        train_stances: list, of ints\n",
        "        bow_vectorizer: sklearn CountVectorizer\n",
        "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
        "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGU4SYsgqfmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define relevant functions\n",
        "def pipeline_train(train, test, lim_unigram):\n",
        "\n",
        "    # Initialise\n",
        "    heads = []\n",
        "    heads_track = {}\n",
        "    bodies = []\n",
        "    bodies_track = {}\n",
        "    body_ids = []\n",
        "    id_ref = {}\n",
        "    train_set = []\n",
        "    train_stances = []\n",
        "    cos_track = {}\n",
        "    test_heads = []\n",
        "    test_heads_track = {}\n",
        "    test_bodies = []\n",
        "    test_bodies_track = {}\n",
        "    test_body_ids = []\n",
        "    head_tfidf_track = {}\n",
        "    body_tfidf_track = {}\n",
        "\n",
        "    # Identify unique heads and bodies\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            heads.append(head)\n",
        "            heads_track[head] = 1\n",
        "        if body_id not in bodies_track:\n",
        "            bodies.append(train.bodies[body_id])\n",
        "            bodies_track[body_id] = 1\n",
        "            body_ids.append(body_id)\n",
        "\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in test_heads_track:\n",
        "            test_heads.append(head)\n",
        "            test_heads_track[head] = 1\n",
        "        if body_id not in test_bodies_track:\n",
        "            test_bodies.append(test.bodies[body_id])\n",
        "            test_bodies_track[body_id] = 1\n",
        "            test_body_ids.append(body_id)\n",
        "\n",
        "    # Create reference dictionary\n",
        "    for i, elem in enumerate(heads + body_ids):\n",
        "        id_ref[elem] = i\n",
        "\n",
        "    # Create vectorizers and BOW and TF arrays for train set\n",
        "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
        "    bow = bow_vectorizer.fit_transform(heads + bodies)  # Train set only\n",
        "\n",
        "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
        "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
        "        fit(heads + bodies + test_heads + test_bodies)  # Train and test sets\n",
        "\n",
        "    # Process train set\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        head_tf = tfreq[id_ref[head]].reshape(1, -1)\n",
        "        body_tf = tfreq[id_ref[body_id]].reshape(1, -1)\n",
        "        if head not in head_tfidf_track:\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray()\n",
        "            head_tfidf_track[head] = head_tfidf\n",
        "        else:\n",
        "            head_tfidf = head_tfidf_track[head]\n",
        "        if body_id not in body_tfidf_track:\n",
        "            body_tfidf = tfidf_vectorizer.transform([train.bodies[body_id]]).toarray()\n",
        "            body_tfidf_track[body_id] = body_tfidf\n",
        "        else:\n",
        "            body_tfidf = body_tfidf_track[body_id]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        train_set.append(feat_vec)\n",
        "        train_stances.append(label_ref[instance['Stance']])\n",
        "\n",
        "    return train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciGuF4UsP-NU",
        "colab_type": "text"
      },
      "source": [
        "\"\"\"\n",
        "    Process test set\n",
        "    Args:\n",
        "        test: ModelData object, test set\n",
        "        bow_vectorizer: sklearn CountVectorizer\n",
        "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
        "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
        "    Returns:\n",
        "        test_set: list, of numpy arrays\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2ZBZZOmryjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline_test(test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer):\n",
        "\n",
        "\n",
        "    # Initialise\n",
        "    test_set = []\n",
        "    heads_track = {}\n",
        "    bodies_track = {}\n",
        "    cos_track = {}\n",
        "\n",
        "    # Process test set\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            head_bow = bow_vectorizer.transform([head]).toarray()\n",
        "            head_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray().reshape(1, -1)\n",
        "            heads_track[head] = (head_tf, head_tfidf)\n",
        "        else:\n",
        "            head_tf = heads_track[head][0]\n",
        "            head_tfidf = heads_track[head][1]\n",
        "        if body_id not in bodies_track:\n",
        "            body_bow = bow_vectorizer.transform([test.bodies[body_id]]).toarray()\n",
        "            body_tf = tfreq_vectorizer.transform(body_bow).toarray()[0].reshape(1, -1)\n",
        "            body_tfidf = tfidf_vectorizer.transform([test.bodies[body_id]]).toarray().reshape(1, -1)\n",
        "            bodies_track[body_id] = (body_tf, body_tfidf)\n",
        "        else:\n",
        "            body_tf = bodies_track[body_id][0]\n",
        "            body_tfidf = bodies_track[body_id][1]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        test_set.append(feat_vec)\n",
        "\n",
        "    return test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__WIsHvPVdUp",
        "colab_type": "text"
      },
      "source": [
        "\"\"\"\n",
        "    Load TensorFlow model\n",
        "    Args:\n",
        "        sess: TensorFlow session\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6NDu23urtWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(sess):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, './model/model.checkpoint')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLBUPGypVidi",
        "colab_type": "text"
      },
      "source": [
        "\"\"\"\n",
        "    Save predictions to CSV file\n",
        "    Args:\n",
        "        pred: numpy array, of numeric predictions\n",
        "        file: str, filename + extension\n",
        "    \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oltCV5Y3rnMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_predictions(pred, file):\n",
        "\n",
        "    with open(file, 'w') as csvfile:\n",
        "        fieldnames = ['Stance']\n",
        "        writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for instance in pred:\n",
        "            writer.writerow({'Stance': label_ref_rev[instance]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTUdMwbSV45V",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDqNvpx0Vrdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ1EalopV960",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4C6MNfTWK1V",
        "colab_type": "code",
        "outputId": "4ec04c82-e571-457b-d1d6-719243228bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Prompt for mode\n",
        "mode = input('mode (load / train)? ')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mode (load / train)? train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i6806hpWQzk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaiWTfi_WYgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set file names\n",
        "file_train_instances = \"train_stances.csv\"\n",
        "file_train_bodies = \"train_bodies.csv\"\n",
        "file_test_instances = \"test_stances_unlabeled.csv\"\n",
        "file_test_bodies = \"test_bodies.csv\"\n",
        "file_predictions = 'predictions_test.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08zVuG-gWc_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise hyperparameters\n",
        "r = random.Random()\n",
        "lim_unigram = 5000\n",
        "target_size = 4\n",
        "hidden_size = 100\n",
        "train_keep_prob = 0.6\n",
        "l2_alpha = 0.00001\n",
        "learn_rate = 0.01\n",
        "clip_ratio = 5\n",
        "batch_size_train = 500\n",
        "epochs = 90"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUb5NMqRWgR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data sets\n",
        "raw_train = ModelData(file_train_instances, file_train_bodies)\n",
        "raw_test = ModelData(file_test_instances, file_test_bodies)\n",
        "n_train = len(raw_train.instances)\n",
        "\n",
        "\n",
        "# Process data sets\n",
        "train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = \\\n",
        "    pipeline_train(raw_train, raw_test, lim_unigram=lim_unigram)\n",
        "feature_size = len(train_set[0])\n",
        "test_set = pipeline_test(raw_test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfkPfADrWmgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJFpkQCKXJ0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "bf38f131-5457-4d6e-cc39-42cf34b09672"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT9tw1GVXJ0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df = raw_train\n",
        "features_train = train_set\n",
        "labels_train = train_stances\n",
        "features_test = test_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR0itis0XJ0S",
        "colab_type": "code",
        "outputId": "7283e582-06e0-4fa8-8928-194066f798a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "rf_0 = RandomForestClassifier(random_state = 8)\n",
        "\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(rf_0.get_params())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters currently in use:\n",
            "\n",
            "{'bootstrap': True,\n",
            " 'ccp_alpha': 0.0,\n",
            " 'class_weight': None,\n",
            " 'criterion': 'gini',\n",
            " 'max_depth': None,\n",
            " 'max_features': 'auto',\n",
            " 'max_leaf_nodes': None,\n",
            " 'max_samples': None,\n",
            " 'min_impurity_decrease': 0.0,\n",
            " 'min_impurity_split': None,\n",
            " 'min_samples_leaf': 1,\n",
            " 'min_samples_split': 2,\n",
            " 'min_weight_fraction_leaf': 0.0,\n",
            " 'n_estimators': 100,\n",
            " 'n_jobs': None,\n",
            " 'oob_score': False,\n",
            " 'random_state': 8,\n",
            " 'verbose': 0,\n",
            " 'warm_start': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0EA3-hJXJ0X",
        "colab_type": "code",
        "outputId": "ee754fbe-63dc-402d-e65b-32c397f865b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "# n_estimators\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n",
        "\n",
        "# max_features\n",
        "max_features = ['auto', 'sqrt']\n",
        "\n",
        "# max_depth\n",
        "max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n",
        "max_depth.append(None)\n",
        "\n",
        "# min_samples_split\n",
        "min_samples_split = [2, 5, 10]\n",
        "\n",
        "# min_samples_leaf\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "# bootstrap\n",
        "bootstrap = [True, False]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "pprint(random_grid)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'bootstrap': [True, False],\n",
            " 'max_depth': [20, 40, 60, 80, 100, None],\n",
            " 'max_features': ['auto', 'sqrt'],\n",
            " 'min_samples_leaf': [1, 2, 4],\n",
            " 'min_samples_split': [2, 5, 10],\n",
            " 'n_estimators': [200, 400, 600, 800, 1000]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRm2EdpWXJ0a",
        "colab_type": "code",
        "outputId": "a7c76d4b-58b9-4774-c832-397482ef4783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# First create the base model to tune\n",
        "rfc = RandomForestClassifier(random_state=8)\n",
        "\n",
        "# Definition of the random search\n",
        "random_search = RandomizedSearchCV(estimator=rfc,\n",
        "                                   param_distributions=random_grid,\n",
        "                                   n_iter=50,\n",
        "                                   scoring='accuracy',\n",
        "                                   cv=3, \n",
        "                                   verbose=1, \n",
        "                                   random_state=8)\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWGfx0KHXJ0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The best hyperparameters from Random Search are:\")\n",
        "print(random_search.best_params_)\n",
        "print(\"\")\n",
        "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
        "print(random_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbi_N5a2XJ0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the parameter grid based on the results of random search \n",
        "bootstrap = [False]\n",
        "max_depth = [30, 40, 50]\n",
        "max_features = ['sqrt']\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "min_samples_split = [5, 10, 15]\n",
        "n_estimators = [800]\n",
        "\n",
        "param_grid = {\n",
        "    'bootstrap': bootstrap,\n",
        "    'max_depth': max_depth,\n",
        "    'max_features': max_features,\n",
        "    'min_samples_leaf': min_samples_leaf,\n",
        "    'min_samples_split': min_samples_split,\n",
        "    'n_estimators': n_estimators\n",
        "}\n",
        "\n",
        "# Create a base model\n",
        "rfc = RandomForestClassifier(random_state=8)\n",
        "\n",
        "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
        "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator=rfc, \n",
        "                           param_grid=param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=cv_sets,\n",
        "                           verbose=1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHoMHe6_XJ0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The best hyperparameters from Grid Search are:\")\n",
        "print(grid_search.best_params_)\n",
        "print(\"\")\n",
        "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
        "print(grid_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forv7ucfXJ0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_rfc = grid_search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czfmcbQwXJ0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_rfc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljwr9ibFXJ01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_rfc.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu_ACf2GXJ04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rfc_pred = best_rfc.predict(features_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGlWXAwHXJ09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training accuracy\n",
        "print(\"The training accuracy is: \")\n",
        "print(accuracy_score(labels_train, best_rfc.predict(features_train)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98Bj8nSbXJ0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test accuracy\n",
        "print(\"The test accuracy is: \")\n",
        "print(accuracy_score(labels_test, rfc_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr9a_1QvXJ1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classification report\n",
        "print(\"Classification report\")\n",
        "print(classification_report(labels_test,rfc_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOfLySOAXJ1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aux_df = df[['Stance', 'Category_Code']].drop_duplicates().sort_values('Category_Code')\n",
        "conf_matrix = confusion_matrix(labels_test, rfc_pred)\n",
        "plt.figure(figsize=(12.8,6))\n",
        "sns.heatmap(conf_matrix, \n",
        "            annot=True,\n",
        "            xticklabels=aux_df['Stance'].values, \n",
        "            yticklabels=aux_df['Stance'].values,\n",
        "            cmap=\"Blues\")\n",
        "plt.ylabel('Predicted')\n",
        "plt.xlabel('Actual')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BZRFd9mXJ1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = RandomForestClassifier(random_state = 8)\n",
        "base_model.fit(features_train, labels_train)\n",
        "accuracy_score(labels_test, base_model.predict(features_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkiT0hc8XJ1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {\n",
        "     'Model': 'Random Forest',\n",
        "     'Training Set Accuracy': accuracy_score(labels_train, best_rfc.predict(features_train)),\n",
        "     'Test Set Accuracy': accuracy_score(labels_test, rfc_pred)\n",
        "}\n",
        "\n",
        "df_models_rfc = pd.DataFrame(d, index=[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNqM96sLXJ1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_models_rfc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvci44MbXJ1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('Models/best_rfc.pickle', 'wb') as output:\n",
        "    pickle.dump(best_rfc, output)\n",
        "    \n",
        "with open('Models/df_models_rfc.pickle', 'wb') as output:\n",
        "    pickle.dump(df_models_rfc, output)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
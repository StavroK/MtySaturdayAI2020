{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ridge Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEpE9uUDJK0Arpa8Emkdw3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StavroK/MtySaturdayAI2020/blob/master/Ridge_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkkN9-pKrAWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant packages and modules\n",
        "from csv import DictReader\n",
        "from csv import DictWriter\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.layers import  Input, Embedding, Dot, Reshape, Dense\n",
        "from tensorflow.python.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_D6amVJXils",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvv0_OMdrMkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise global variables\n",
        "label_ref = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
        "label_ref_rev = {0: 'agree', 1: 'disagree', 2: 'discuss', 3: 'unrelated'}\n",
        "stop_words = [\n",
        "        \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
        "        \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
        "        \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\",\n",
        "        \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
        "        \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"co\",\n",
        "        \"con\", \"could\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\",\n",
        "        \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
        "        \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\",\n",
        "        \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\",\n",
        "        \"has\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
        "        \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\",\n",
        "        \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
        "        \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\",\n",
        "        \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"now\", \"nowhere\",\n",
        "        \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
        "        \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\n",
        "        \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\",\n",
        "        \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\",\n",
        "        \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
        "        \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\",\n",
        "        \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\",\n",
        "        \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\",\n",
        "        \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\",\n",
        "        \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\",\n",
        "        \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjo4pCpdq_B2",
        "colab_type": "text"
      },
      "source": [
        "Define class for Example data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzYtqbHqsgHm",
        "colab_type": "text"
      },
      "source": [
        "Read Example data from CSV file.\n",
        "---\n",
        "            - **INPUTS**  -> filename: str, filename + extension\n",
        "            - **OUTPUTS** -> rows: list, of dict per instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYFDKUcQrRyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define data class\n",
        "class ModelData:\n",
        "\n",
        "    def __init__(self, file_instances, file_bodies):\n",
        "\n",
        "        # Load data\n",
        "        self.instances = self.read(file_instances)\n",
        "        bodies = self.read(file_bodies)\n",
        "        self.heads = {}\n",
        "        self.bodies = {}\n",
        "\n",
        "        # Process instances\n",
        "        for instance in self.instances:\n",
        "            if instance['Headline'] not in self.heads:\n",
        "                head_id = len(self.heads)\n",
        "                self.heads[instance['Headline']] = head_id\n",
        "            instance['Body ID'] = int(instance['Body ID'])\n",
        "\n",
        "        # Process bodies\n",
        "        for body in bodies:\n",
        "            self.bodies[int(body['Body ID'])] = body['articleBody']\n",
        "\n",
        "    def read(self, filename):\n",
        "\n",
        "\n",
        "\n",
        "        # Initialise\n",
        "        rows = []\n",
        "\n",
        "        # Process file\n",
        "        with open(filename, \"r\", encoding='utf-8') as table:\n",
        "            r = DictReader(table)\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "\n",
        "        return rows\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7sekX1dxgcL",
        "colab_type": "text"
      },
      "source": [
        "    \"\"\"\n",
        "    Process train set, create relevant vectorizers\n",
        "    Args:\n",
        "        train: ModelData object, train set\n",
        "        test: ModelData object, test set\n",
        "        lim_unigram: int, number of most frequent words to consider\n",
        "    Returns:\n",
        "        train_set: list, of numpy arrays\n",
        "        train_stances: list, of ints\n",
        "        bow_vectorizer: sklearn CountVectorizer\n",
        "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
        "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGU4SYsgqfmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define relevant functions\n",
        "def pipeline_train(train, test, lim_unigram):\n",
        "\n",
        "    # Initialise\n",
        "    heads = []\n",
        "    heads_track = {}\n",
        "    bodies = []\n",
        "    bodies_track = {}\n",
        "    body_ids = []\n",
        "    id_ref = {}\n",
        "    train_set = []\n",
        "    train_stances = []\n",
        "    cos_track = {}\n",
        "    test_heads = []\n",
        "    test_heads_track = {}\n",
        "    test_bodies = []\n",
        "    test_bodies_track = {}\n",
        "    test_body_ids = []\n",
        "    head_tfidf_track = {}\n",
        "    body_tfidf_track = {}\n",
        "\n",
        "    # Identify unique heads and bodies\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            heads.append(head)\n",
        "            heads_track[head] = 1\n",
        "        if body_id not in bodies_track:\n",
        "            bodies.append(train.bodies[body_id])\n",
        "            bodies_track[body_id] = 1\n",
        "            body_ids.append(body_id)\n",
        "\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in test_heads_track:\n",
        "            test_heads.append(head)\n",
        "            test_heads_track[head] = 1\n",
        "        if body_id not in test_bodies_track:\n",
        "            test_bodies.append(test.bodies[body_id])\n",
        "            test_bodies_track[body_id] = 1\n",
        "            test_body_ids.append(body_id)\n",
        "\n",
        "    # Create reference dictionary\n",
        "    for i, elem in enumerate(heads + body_ids):\n",
        "        id_ref[elem] = i\n",
        "\n",
        "    # Create vectorizers and BOW and TF arrays for train set\n",
        "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
        "    bow = bow_vectorizer.fit_transform(heads + bodies)  # Train set only\n",
        "\n",
        "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
        "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
        "        fit(heads + bodies + test_heads + test_bodies)  # Train and test sets\n",
        "\n",
        "    # Process train set\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        head_tf = tfreq[id_ref[head]].reshape(1, -1)\n",
        "        body_tf = tfreq[id_ref[body_id]].reshape(1, -1)\n",
        "        if head not in head_tfidf_track:\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray()\n",
        "            head_tfidf_track[head] = head_tfidf\n",
        "        else:\n",
        "            head_tfidf = head_tfidf_track[head]\n",
        "        if body_id not in body_tfidf_track:\n",
        "            body_tfidf = tfidf_vectorizer.transform([train.bodies[body_id]]).toarray()\n",
        "            body_tfidf_track[body_id] = body_tfidf\n",
        "        else:\n",
        "            body_tfidf = body_tfidf_track[body_id]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        train_set.append(feat_vec)\n",
        "        train_stances.append(label_ref[instance['Stance']])\n",
        "\n",
        "    return train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciGuF4UsP-NU",
        "colab_type": "text"
      },
      "source": [
        "\"\"\"\n",
        "    Process test set\n",
        "    Args:\n",
        "        test: ModelData object, test set\n",
        "        bow_vectorizer: sklearn CountVectorizer\n",
        "        tfreq_vectorizer: sklearn TfidfTransformer(use_idf=False)\n",
        "        tfidf_vectorizer: sklearn TfidfVectorizer()\n",
        "    Returns:\n",
        "        test_set: list, of numpy arrays\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2ZBZZOmryjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pipeline_test(test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer):\n",
        "\n",
        "\n",
        "    # Initialise\n",
        "    test_set = []\n",
        "    heads_track = {}\n",
        "    bodies_track = {}\n",
        "    cos_track = {}\n",
        "\n",
        "    # Process test set\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            head_bow = bow_vectorizer.transform([head]).toarray()\n",
        "            head_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray().reshape(1, -1)\n",
        "            heads_track[head] = (head_tf, head_tfidf)\n",
        "        else:\n",
        "            head_tf = heads_track[head][0]\n",
        "            head_tfidf = heads_track[head][1]\n",
        "        if body_id not in bodies_track:\n",
        "            body_bow = bow_vectorizer.transform([test.bodies[body_id]]).toarray()\n",
        "            body_tf = tfreq_vectorizer.transform(body_bow).toarray()[0].reshape(1, -1)\n",
        "            body_tfidf = tfidf_vectorizer.transform([test.bodies[body_id]]).toarray().reshape(1, -1)\n",
        "            bodies_track[body_id] = (body_tf, body_tfidf)\n",
        "        else:\n",
        "            body_tf = bodies_track[body_id][0]\n",
        "            body_tfidf = bodies_track[body_id][1]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        test_set.append(feat_vec)\n",
        "\n",
        "    return test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__WIsHvPVdUp",
        "colab_type": "text"
      },
      "source": [
        "\"\"\"\n",
        "    Load TensorFlow model\n",
        "    Args:\n",
        "        sess: TensorFlow session\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6NDu23urtWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(sess):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, './model/model.checkpoint')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLBUPGypVidi",
        "colab_type": "text"
      },
      "source": [
        "\"\"\"\n",
        "    Save predictions to CSV file\n",
        "    Args:\n",
        "        pred: numpy array, of numeric predictions\n",
        "        file: str, filename + extension\n",
        "    \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oltCV5Y3rnMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_predictions(pred, file):\n",
        "\n",
        "    with open(file, 'w') as csvfile:\n",
        "        fieldnames = ['Stance']\n",
        "        writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for instance in pred:\n",
        "            writer.writerow({'Stance': label_ref_rev[instance]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaiWTfi_WYgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set file names\n",
        "file_train_instances = \"train_stances.csv\"\n",
        "file_train_bodies = \"train_bodies.csv\"\n",
        "file_test_instances = \"test_stances_unlabeled.csv\"\n",
        "file_test_bodies = \"test_bodies.csv\"\n",
        "file_predictions = 'predictions_test.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08zVuG-gWc_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialise hyperparameters\n",
        "r = random.Random()\n",
        "lim_unigram = 5000\n",
        "target_size = 4\n",
        "hidden_size = 100\n",
        "train_keep_prob = 0.6\n",
        "l2_alpha = 0.00001\n",
        "learn_rate = 0.01\n",
        "clip_ratio = 5\n",
        "batch_size_train = 500\n",
        "epochs = 90"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUb5NMqRWgR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data sets\n",
        "raw_train = ModelData(file_train_instances, file_train_bodies)\n",
        "raw_test = ModelData(file_test_instances, file_test_bodies)\n",
        "n_train = len(raw_train.instances)\n",
        "\n",
        "\n",
        "# Process data sets\n",
        "train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = \\\n",
        "    pipeline_train(raw_train, raw_test, lim_unigram=lim_unigram)\n",
        "feature_size = len(train_set[0])\n",
        "test_set = pipeline_test(raw_test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfkPfADrWmgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJFpkQCKXJ0E",
        "colab_type": "code",
        "outputId": "b17a476d-9807-4009-8478-64883058dcb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT9tw1GVXJ0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df = raw_train\n",
        "features_train = train_set\n",
        "labels_train = train_stances\n",
        "features_test = test_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGLxzqRoGVU",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation for Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klP4fBJFoGVU",
        "colab_type": "text"
      },
      "source": [
        "First, we can see what hyperparameters the model has:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS7lRDgCoGVV",
        "colab_type": "code",
        "outputId": "1c29cfac-bfa3-47b2-adcc-a7bca35a127c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "lr_0 = LogisticRegression(random_state = 8)\n",
        "\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(lr_0.get_params())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters currently in use:\n",
            "\n",
            "{'C': 1.0,\n",
            " 'class_weight': None,\n",
            " 'dual': False,\n",
            " 'fit_intercept': True,\n",
            " 'intercept_scaling': 1,\n",
            " 'l1_ratio': None,\n",
            " 'max_iter': 100,\n",
            " 'multi_class': 'auto',\n",
            " 'n_jobs': None,\n",
            " 'penalty': 'l2',\n",
            " 'random_state': 8,\n",
            " 'solver': 'lbfgs',\n",
            " 'tol': 0.0001,\n",
            " 'verbose': 0,\n",
            " 'warm_start': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhtCvA-SoGVX",
        "colab_type": "text"
      },
      "source": [
        "We'll tune the following ones:\n",
        "\n",
        "* `C` = Inverse of regularization strength. Smaller values specify stronger regularization.\n",
        "* `multi_class` = We'll choose `multinomial` because this is a multi-class problem.\n",
        "* `solver` = Algorithm to use in the optimization problem. For multiclass problems, only `newton-cg`, `sag`, `saga` and `lbfgs` handle multinomial loss.\n",
        "* `class_weight`: Weights associated with classes. \n",
        "* `penalty`: Used to specify the norm used in the penalization. The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcn9nINboGVX",
        "colab_type": "text"
      },
      "source": [
        "### Randomized Search Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpZbbfkOoGVY",
        "colab_type": "text"
      },
      "source": [
        "We first need to define the grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS_0rWwEoGVY",
        "colab_type": "code",
        "outputId": "439835c4-15df-474a-9564-e9dc12c3f97d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "# C\n",
        "C = [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 10)]\n",
        "\n",
        "# multi_class\n",
        "multi_class = ['multinomial']\n",
        "\n",
        "# solver\n",
        "solver = ['newton-cg', 'sag', 'saga', 'lbfgs']\n",
        " \n",
        "# class_weight\n",
        "class_weight = ['balanced', None]\n",
        "\n",
        "# penalty\n",
        "penalty = ['l2']\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'C': C,\n",
        "               'multi_class': multi_class,\n",
        "               'solver': solver,\n",
        "               'class_weight': class_weight,\n",
        "               'penalty': penalty}\n",
        "\n",
        "pprint(random_grid)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': [0.1,\n",
            "       0.2,\n",
            "       0.30000000000000004,\n",
            "       0.4,\n",
            "       0.5,\n",
            "       0.6,\n",
            "       0.7000000000000001,\n",
            "       0.8,\n",
            "       0.9,\n",
            "       1.0],\n",
            " 'class_weight': ['balanced', None],\n",
            " 'multi_class': ['multinomial'],\n",
            " 'penalty': ['l2'],\n",
            " 'solver': ['newton-cg', 'sag', 'saga', 'lbfgs']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBEB6EUaoGVa",
        "colab_type": "text"
      },
      "source": [
        "Then, we'll perform the Random Search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8yqrU8joGVb",
        "colab_type": "code",
        "outputId": "0868cf9e-39ed-497c-b7fc-703e0820a35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "# First create the base model to tune\n",
        "lrc = LogisticRegression(random_state=8)\n",
        "\n",
        "# Definition of the random search\n",
        "random_search = RandomizedSearchCV(estimator=lrc,\n",
        "                                   param_distributions=random_grid,\n",
        "                                   n_iter=50,\n",
        "                                   scoring='accuracy',\n",
        "                                   cv=3, \n",
        "                                   verbose=1, \n",
        "                                   random_state=8)\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdPt589yoGVd",
        "colab_type": "text"
      },
      "source": [
        "We can see the best hyperparameters resulting from the Random Search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3HsADz_oGVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The best hyperparameters from Random Search are:\")\n",
        "print(random_search.best_params_)\n",
        "print(\"\")\n",
        "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
        "print(random_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clvVb-P-oGVf",
        "colab_type": "text"
      },
      "source": [
        "After that, we can do a more exhaustive search centered in those values:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5ws0pO_oGVg",
        "colab_type": "text"
      },
      "source": [
        "### Grid Search Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXxCESnGoGVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the parameter grid based on the results of random search \n",
        "C = [float(x) for x in np.linspace(start = 0.6, stop = 1, num = 10)]\n",
        "multi_class = ['multinomial']\n",
        "solver = ['sag']\n",
        "class_weight = ['balanced']\n",
        "penalty = ['l2']\n",
        "\n",
        "param_grid = {'C': C,\n",
        "               'multi_class': multi_class,\n",
        "               'solver': solver,\n",
        "               'class_weight': class_weight,\n",
        "               'penalty': penalty}\n",
        "\n",
        "# Create a base model\n",
        "lrc = LogisticRegression(random_state=8)\n",
        "\n",
        "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
        "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator=lrc, \n",
        "                           param_grid=param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=cv_sets,\n",
        "                           verbose=1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbrdbzavoGVi",
        "colab_type": "text"
      },
      "source": [
        "The best hyperparameters turn out to be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzDB44l0oGVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The best hyperparameters from Grid Search are:\")\n",
        "print(grid_search.best_params_)\n",
        "print(\"\")\n",
        "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
        "print(grid_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4XUlnDboGVl",
        "colab_type": "text"
      },
      "source": [
        "Let's save the model in `best_lrc`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyL73BeSoGVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_lrc = grid_search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox3PfPn0oGVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_lrc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNaB6t3coGVp",
        "colab_type": "text"
      },
      "source": [
        "We now know the best logistic regression model. Let's fit it and see how it performs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whZSreH9oGVp",
        "colab_type": "text"
      },
      "source": [
        "## Model fit and performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLcEb_wToGVp",
        "colab_type": "text"
      },
      "source": [
        "Now, we can fit the model to our training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER-c7OmGoGVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_lrc.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW5NdHyCoGVs",
        "colab_type": "text"
      },
      "source": [
        "And get the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM7DYa_AoGVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrc_pred = best_lrc.predict(features_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NoYQ_8oGVu",
        "colab_type": "text"
      },
      "source": [
        "The conditional class probabilities can be obtained by typing:\n",
        "\n",
        "`lrc_pred = best_lrc.predict_proba(features_test)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdKWWN4hoGVv",
        "colab_type": "text"
      },
      "source": [
        "For performance analysis, we will use the confusion matrix, the classification report and the accuracy on both training and test data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqTEaaORoGVv",
        "colab_type": "text"
      },
      "source": [
        "#### Training accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyvFAOHboGVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training accuracy\n",
        "print(\"The training accuracy is: \")\n",
        "print(accuracy_score(labels_train, best_lrc.predict(features_train)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW-xc8I8oGVy",
        "colab_type": "text"
      },
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no_6HqR2oGVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test accuracy\n",
        "print(\"The test accuracy is: \")\n",
        "print(accuracy_score(labels_test, lrc_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1Av44xoGV0",
        "colab_type": "text"
      },
      "source": [
        "#### Classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZh8wKi0oGV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classification report\n",
        "print(\"Classification report\")\n",
        "print(classification_report(labels_test,lrc_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzQVYudYoGV2",
        "colab_type": "text"
      },
      "source": [
        "#### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI1CKvEyoGV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aux_df = df[['Category', 'Category_Code']].drop_duplicates().sort_values('Category_Code')\n",
        "conf_matrix = confusion_matrix(labels_test, lrc_pred)\n",
        "plt.figure(figsize=(12.8,6))\n",
        "sns.heatmap(conf_matrix, \n",
        "            annot=True,\n",
        "            xticklabels=aux_df['Category'].values, \n",
        "            yticklabels=aux_df['Category'].values,\n",
        "            cmap=\"Blues\")\n",
        "plt.ylabel('Predicted')\n",
        "plt.xlabel('Actual')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3PrBcQnoGV4",
        "colab_type": "text"
      },
      "source": [
        "At this point, we could get the average time the model takes to get predictions. We want the algorithm to be fast since we are creating an app which will gather data from the internet and get the predicted categories. However, since the difference when predicting 10-20 observations will be very little, we won't take this into account.\n",
        "\n",
        "However, the code below could do this task:\n",
        "\n",
        "```python\n",
        "features_time = features_train\n",
        "elapsed_list = []\n",
        "for i in range(0,10):\n",
        "    \n",
        "    start = time.time()\n",
        "    predictions = best_lrc.predict(features_time)\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    elapsed_list.append(elapsed)\n",
        "\n",
        "mean_time_elapsed = np.mean(elapsed_list)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-atjwLC8oGV4",
        "colab_type": "text"
      },
      "source": [
        "Let's see if the hyperparameter tuning process has returned a better model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTTq-P-yoGV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = LogisticRegression(random_state = 8)\n",
        "base_model.fit(features_train, labels_train)\n",
        "accuracy_score(labels_test, base_model.predict(features_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43uAleu3oGV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_lrc.fit(features_train, labels_train)\n",
        "accuracy_score(labels_test, best_lrc.predict(features_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG3hS5AioGV8",
        "colab_type": "text"
      },
      "source": [
        "We get approximately the same accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqxL_DqnoGV9",
        "colab_type": "text"
      },
      "source": [
        "We'll create a dataset with a model summary to compare models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKj-reHDoGV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {\n",
        "     'Model': 'Logistic Regression',\n",
        "     'Training Set Accuracy': accuracy_score(labels_train, best_lrc.predict(features_train)),\n",
        "     'Test Set Accuracy': accuracy_score(labels_test, lrc_pred)\n",
        "}\n",
        "\n",
        "df_models_lrc = pd.DataFrame(d, index=[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4QHokPeoGWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_models_lrc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVZfPMRCoGWC",
        "colab_type": "text"
      },
      "source": [
        "Let's save the model and this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xc9cFimoGWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('Models/best_lrc.pickle', 'wb') as output:\n",
        "    pickle.dump(best_lrc, output)\n",
        "    \n",
        "with open('Models/df_models_lrc.pickle', 'wb') as output:\n",
        "    pickle.dump(df_models_lrc, output)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
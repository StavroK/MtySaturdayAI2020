{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fakenews Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPW6rRZ2T2KRmjaGtJ8RKgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StavroK/MtySaturdayAI2020/blob/master/Fakenews_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mevw_bE_Wl8m",
        "colab_type": "text"
      },
      "source": [
        "**Data Set Analysis**\n",
        "\n",
        "In this step we will check for a balanced data so we can train the machine learning models without oversampling or undersampling classes. If we see some bias towards one class we need to apply techniques to manage this issue with our data set.\n",
        "\n",
        "First step is to check if we have all Python packages we need be using to visualize the characteristics of the dataset to check for class balance, and install those missing if any. We will need to join files containing labeling of examples to content of articles for our purpose.\n",
        "\n",
        "\n",
        "We want to check for: 1. Number of examples in each class, 2. Total Percentage of examples in each class, 3. Distribution of the lenght of each example as we will use its word content to train for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAENgkVySxye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This command will show you the \"Location\" of your package\n",
        "# I am using numpy as is the most common package \n",
        "! pip show numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17tVWgflbQVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this other command to list all files under \"Location\"\n",
        "# we are checking for the following packages\n",
        "# altair - enables to build histogram for class representation in examples\n",
        "# matplotlib - extension of NumPy enables you to embed plots into applications\n",
        "# pandas - used to clean, transform, manipulate and analize data\n",
        "# pickle - used to convert a python object into a character stream\n",
        "# pip - used to manage installation and updates of python packages\n",
        "# seaborn - based on matplotlib is used to vizualize example lenght by category \n",
        "# warnings - used to hide warinings coming from seaborn package\n",
        "! ls /usr/local/lib/python3.6/dist-packages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sojJxGrscdYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you are missign a package use this command to install\n",
        "# remember to be in your distribution packages folder used by colab \n",
        "# before you run the instalation command\n",
        "\n",
        "%cd /usr/local/lib/python3.6/dist-packages\n",
        "\n",
        "# use these commands to install and check installation\n",
        "# ! pip install nameofpackage\n",
        "# ! pip show nameofpackage\n",
        "\n",
        "! pip install warn\n",
        "! pip show warn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAGDPipevMsl",
        "colab_type": "text"
      },
      "source": [
        "Now that we have secured environment requirements for python packages that will be declared in the code, next we will import them into the program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ZntwLOvMfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import altair as alt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import seaborn as sns\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6bR0056rHOL",
        "colab_type": "text"
      },
      "source": [
        "We want to check for: 1. Number of examples in each class, 2. Total Percentage of examples in each class, 3. Distribution of the lenght of each example as we will use its word content to train for classification.\n",
        "\n",
        "Data is already included in /content/FakeNews folder. Data is already separated by example content and labeled files, and also in train, test and demo files to keep process clean.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-k8pLG1rUtU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "56aa73ed-c281-4bd1-a09f-f50396f025fa"
      },
      "source": [
        "%cd /content/FakeNews/\n",
        "\n",
        "file_train_instances = \"train_stances.csv\"\n",
        "file_train_bodies = \"train_bodies.csv\"\n",
        "file_test_instances = \"test_stances_unlabeled.csv\"\n",
        "file_test_bodies = \"test_bodies.csv\"\n",
        "file_predictions = 'predictions_test.csv'\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/FakeNews\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S9oj2QQmTca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "222507c1-d4e9-450b-cd39-693d54b8fb27"
      },
      "source": [
        "%cd /content/FakeNews/\n",
        "\"\"\"\n",
        "Scorer for the Fake News Challenge\n",
        " - @bgalbraith\n",
        "Submission is a CSV with the following fields: Headline, Body ID, Stance\n",
        "where Stance is in {agree, disagree, discuss, unrelated}\n",
        "Scoring is as follows:\n",
        "  +0.25 for each correct unrelated\n",
        "  +0.25 for each correct related (label is any of agree, disagree, discuss)\n",
        "  +0.75 for each correct agree, disagree, discuss\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "\n",
        "FIELDNAMES = ['Headline', 'Body ID', 'Stance']\n",
        "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "RELATED = LABELS[0:3]\n",
        "\n",
        "USAGE = \"\"\"\n",
        "FakeNewsChallenge FNC-1 scorer - version 1.0\n",
        "Usage: python scorer.py gold_labels test_labels\n",
        "  gold_labels - CSV file with reference GOLD stance labels\n",
        "  test_labels - CSV file with predicted stance labels\n",
        "The scorer will provide three scores: MAX, NULL, and TEST\n",
        "  MAX  - the best possible score (100% accuracy)\n",
        "  NULL - score as if all predicted stances were unrelated\n",
        "  TEST - score based on the provided predictions\n",
        "\"\"\"\n",
        "\n",
        "ERROR_MISMATCH = \"\"\"\n",
        "ERROR: Entry mismatch at line {}\n",
        " [expected] Headline: {} // Body ID: {}\n",
        " [got] Headline: {} // Body ID: {}\n",
        "\"\"\"\n",
        "\n",
        "SCORE_REPORT = \"\"\"\n",
        "MAX  - the best possible score (100% accuracy)\n",
        "NULL - score as if all predicted stances were unrelated\n",
        "TEST - score based on the provided predictions\n",
        "||    MAX    ||    NULL   ||    TEST   ||\\n||{:^11}||{:^11}||{:^11}||\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FNCException(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def score_submission(gold_labels, test_labels):\n",
        "    score = 0.0\n",
        "    cm = [[0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0],\n",
        "          [0, 0, 0, 0]]\n",
        "\n",
        "    for i, (g, t) in enumerate(zip(gold_labels, test_labels)):\n",
        "        if g['Headline'] != t['Headline'] or g['Body ID'] != t['Body ID']:\n",
        "            error = ERROR_MISMATCH.format(i+2,\n",
        "                                          g['Headline'], g['Body ID'],\n",
        "                                          t['Headline'], t['Body ID'])\n",
        "            raise FNCException(error)\n",
        "        else:\n",
        "            g_stance, t_stance = g['Stance'], t['Stance']\n",
        "            if g_stance == t_stance:\n",
        "                score += 0.25\n",
        "                if g_stance != 'unrelated':\n",
        "                    score += 0.50\n",
        "            if g_stance in RELATED and t_stance in RELATED:\n",
        "                score += 0.25\n",
        "\n",
        "        cm[LABELS.index(g_stance)][LABELS.index(t_stance)] += 1\n",
        "\n",
        "    return score, cm\n",
        "\n",
        "\n",
        "def score_defaults(gold_labels):\n",
        "    \"\"\"\n",
        "    Compute the \"all false\" baseline (all labels as unrelated) and the max\n",
        "    possible score\n",
        "    :param gold_labels: list containing the true labels\n",
        "    :return: (null_score, best_score)\n",
        "    \"\"\"\n",
        "    unrelated = [g for g in gold_labels if g['Stance'] == 'unrelated']\n",
        "    null_score = 0.25 * len(unrelated)\n",
        "    max_score = null_score + (len(gold_labels) - len(unrelated))\n",
        "    return null_score, max_score\n",
        "\n",
        "\n",
        "def load_dataset(filename):\n",
        "    data = None\n",
        "    try:\n",
        "        with open(filename) as fh:\n",
        "            reader = csv.DictReader(fh)\n",
        "            if reader.fieldnames != FIELDNAMES:\n",
        "                error = 'ERROR: Incorrect headers in: {}'.format(filename)\n",
        "                raise FNCException(error)\n",
        "            else:\n",
        "                data = list(reader)\n",
        "\n",
        "            if data is None:\n",
        "                error = 'ERROR: No data found in: {}'.format(filename)\n",
        "                raise FNCException(error)\n",
        "    except FileNotFoundError:\n",
        "        error = \"ERROR: Could not find file: {}\".format(filename)\n",
        "        raise FNCException(error)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def print_confusion_matrix(cm):\n",
        "    lines = ['CONFUSION MATRIX:']\n",
        "    header = \"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format('', *LABELS)\n",
        "    line_len = len(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "    lines.append(header)\n",
        "    lines.append(\"-\"*line_len)\n",
        "\n",
        "    hit = 0\n",
        "    total = 0\n",
        "    for i, row in enumerate(cm):\n",
        "        hit += row[i]\n",
        "        total += sum(row)\n",
        "        lines.append(\"|{:^11}|{:^11}|{:^11}|{:^11}|{:^11}|\".format(LABELS[i],\n",
        "                                                                   *row))\n",
        "        lines.append(\"-\"*line_len)\n",
        "    lines.append(\"ACCURACY: {:.3f}\".format(hit / total))\n",
        "    print('\\n'.join(lines))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) != 3:\n",
        "        print(USAGE)\n",
        "        sys.exit(0)\n",
        "\n",
        "    _, gold_filename, test_filename = sys.argv\n",
        "\n",
        "    try:\n",
        "        gold_labels = load_dataset(gold_filename)\n",
        "        test_labels = load_dataset(test_filename)\n",
        "\n",
        "        test_score, cm = score_submission(gold_labels, test_labels)\n",
        "        null_score, max_score = score_defaults(gold_labels)\n",
        "        print_confusion_matrix(cm)\n",
        "        print(SCORE_REPORT.format(max_score, null_score, test_score))\n",
        "\n",
        "    except FNCException as e:\n",
        "        print(e)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/FakeNews\n",
            "ERROR: Could not find file: -f\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}

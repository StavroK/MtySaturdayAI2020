{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-layer Perceptron Classification Model .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlM9pBaOguTTicrt67sgsP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StavroK/MtySaturdayAI2020/blob/master/Multi_layer_Perceptron_Classification_Model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ8754qSP8vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import relevant packages and modules\n",
        "from util import *\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Prompt for mode\n",
        "mode = input('mode (load / train)? ')\n",
        "\n",
        "\n",
        "# Set file names\n",
        "file_train_instances = \"train_stances.csv\"\n",
        "file_train_bodies = \"train_bodies.csv\"\n",
        "file_test_instances = \"test_stances_unlabeled.csv\"\n",
        "file_test_bodies = \"test_bodies.csv\"\n",
        "file_predictions = 'predictions_test.csv'\n",
        "\n",
        "\n",
        "# Initialise hyperparameters\n",
        "r = random.Random()\n",
        "lim_unigram = 5000\n",
        "target_size = 4\n",
        "hidden_size = 100\n",
        "train_keep_prob = 0.6\n",
        "l2_alpha = 0.00001\n",
        "learn_rate = 0.01\n",
        "clip_ratio = 5\n",
        "batch_size_train = 500\n",
        "epochs = 90\n",
        "\n",
        "\n",
        "# Load data sets\n",
        "raw_train = FNCData(file_train_instances, file_train_bodies)\n",
        "raw_test = FNCData(file_test_instances, file_test_bodies)\n",
        "n_train = len(raw_train.instances)\n",
        "\n",
        "\n",
        "# Process data sets\n",
        "train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = \\\n",
        "    pipeline_train(raw_train, raw_test, lim_unigram=lim_unigram)\n",
        "feature_size = len(train_set[0])\n",
        "test_set = pipeline_test(raw_test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer)\n",
        "\n",
        "\n",
        "# Define model\n",
        "\n",
        "# Create placeholders\n",
        "features_pl = tf.placeholder(tf.float32, [None, feature_size], 'features')\n",
        "stances_pl = tf.placeholder(tf.int64, [None], 'stances')\n",
        "keep_prob_pl = tf.placeholder(tf.float32)\n",
        "\n",
        "# Infer batch size\n",
        "batch_size = tf.shape(features_pl)[0]\n",
        "\n",
        "# Define multi-layer perceptron\n",
        "hidden_layer = tf.nn.dropout(tf.nn.relu(tf.contrib.layers.linear(features_pl, hidden_size)), keep_prob=keep_prob_pl)\n",
        "logits_flat = tf.nn.dropout(tf.contrib.layers.linear(hidden_layer, target_size), keep_prob=keep_prob_pl)\n",
        "logits = tf.reshape(logits_flat, [batch_size, target_size])\n",
        "\n",
        "# Define L2 loss\n",
        "tf_vars = tf.trainable_variables()\n",
        "l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf_vars if 'bias' not in v.name]) * l2_alpha\n",
        "\n",
        "# Define overall loss\n",
        "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, stances_pl) + l2_loss)\n",
        "\n",
        "# Define prediction\n",
        "softmaxed_logits = tf.nn.softmax(logits)\n",
        "predict = tf.arg_max(softmaxed_logits, 1)\n",
        "\n",
        "\n",
        "# Load model\n",
        "if mode == 'load':\n",
        "    with tf.Session() as sess:\n",
        "        load_model(sess)\n",
        "\n",
        "\n",
        "        # Predict\n",
        "        test_feed_dict = {features_pl: test_set, keep_prob_pl: 1.0}\n",
        "        test_pred = sess.run(predict, feed_dict=test_feed_dict)\n",
        "\n",
        "\n",
        "# Train model\n",
        "if mode == 'train':\n",
        "\n",
        "    # Define optimiser\n",
        "    opt_func = tf.train.AdamOptimizer(learn_rate)\n",
        "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tf_vars), clip_ratio)\n",
        "    opt_op = opt_func.apply_gradients(zip(grads, tf_vars))\n",
        "\n",
        "    # Perform training\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            indices = list(range(n_train))\n",
        "            r.shuffle(indices)\n",
        "\n",
        "            for i in range(n_train // batch_size_train):\n",
        "                batch_indices = indices[i * batch_size_train: (i + 1) * batch_size_train]\n",
        "                batch_features = [train_set[i] for i in batch_indices]\n",
        "                batch_stances = [train_stances[i] for i in batch_indices]\n",
        "\n",
        "                batch_feed_dict = {features_pl: batch_features, stances_pl: batch_stances, keep_prob_pl: train_keep_prob}\n",
        "                _, current_loss = sess.run([opt_op, loss], feed_dict=batch_feed_dict)\n",
        "                total_loss += current_loss\n",
        "\n",
        "\n",
        "        # Predict\n",
        "        test_feed_dict = {features_pl: test_set, keep_prob_pl: 1.0}\n",
        "        test_pred = sess.run(predict, feed_dict=test_feed_dict)\n",
        "\n",
        "\n",
        "# Save predictions\n",
        "save_predictions(test_pred, file_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
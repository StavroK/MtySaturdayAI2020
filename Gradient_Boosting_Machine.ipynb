{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Gradient Boosting Machine.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StavroK/MtySaturdayAI2020/blob/master/Gradient_Boosting_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS34UHnmQSvE",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Boosting Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHf0I5R3QSvF",
        "colab_type": "text"
      },
      "source": [
        "References: \n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqet4kZgQSvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOWrkZ4PWrVe",
        "colab_type": "text"
      },
      "source": [
        "First, we load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqHAXsJsHXvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "a5f5a982-1726-443c-d365-6afcdcd3374b"
      },
      "source": [
        "data = []\n",
        "train_df=[]\n",
        "trainbodies_df = []\n",
        "trainstances_df = []\n",
        "\n",
        "trainbodies_url = 'https://raw.githubusercontent.com/StavroK/MtySaturdayAI2020/master/train_bodies.csv'\n",
        "trainbodies_df = pd.read_csv(trainbodies_url)\n",
        "trainstances_url = 'https://raw.githubusercontent.com/StavroK/MtySaturdayAI2020/master/train_stances.random.csv'\n",
        "trainstances_df = pd.read_csv(trainstances_url)\n",
        "\n",
        "trainbodies_df.sort_values(by='Body ID')\n",
        "trainstances_df.sort_values(by='Body ID')\n",
        "\n",
        "\n",
        "train_df = pd.merge(trainbodies_df,trainstances_df, on=\"Body ID\")\n",
        "\n",
        "train_df['articleBody_Parsed_1'] = train_df['articleBody'].str.replace(\"\\r\", \" \")\n",
        "train_df['articleBody_Parsed_1'] = train_df['articleBody_Parsed_1'].str.replace(\"\\n\", \" \")\n",
        "train_df['articleBody_Parsed_1'] = train_df['articleBody_Parsed_1'].str.replace(\"    \", \" \")\n",
        "train_df['articleBody_Parsed_1'] = train_df['articleBody_Parsed_1'].str.replace('\"', '')\n",
        "train_df['articleBody_Parsed_2'] = train_df['articleBody_Parsed_1'].str.lower()\n",
        "punctuation_signs = list(\"?:!.,;\")\n",
        "train_df['articleBody_Parsed_3'] = train_df['articleBody_Parsed_2']\n",
        "\n",
        "for punct_sign in punctuation_signs:\n",
        "    train_df['articleBody_Parsed_3'] = train_df['articleBody_Parsed_3'].str.replace(punct_sign, '')\n",
        "\n",
        "train_df['articleBody_Parsed_4'] = train_df['articleBody_Parsed_3'].str.replace(\"'s\", \"\")\n",
        "nltk.download('punkt')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "nrows = len(train_df)\n",
        "lemmatized_text_list = []\n",
        "\n",
        "for row in range(0, nrows):\n",
        "    \n",
        "    lemmatized_list = []\n",
        "    text = train_df.loc[row]['articleBody_Parsed_4']\n",
        "    text_words = text.split(\" \")\n",
        "\n",
        "    for word in text_words:\n",
        "      lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
        "        \n",
        "    lemmatized_text = \" \".join(lemmatized_list)\n",
        "    lemmatized_text_list.append(lemmatized_text)\n",
        "train_df['articleBody_Parsed_5'] = lemmatized_text_list\n",
        "nltk.download('stopwords')\n",
        "stop_words = list(stopwords.words('english'))\n",
        "train_df['articleBody_Parsed_6'] = train_df['articleBody_Parsed_5']\n",
        "\n",
        "for stop_word in stop_words:\n",
        "\n",
        "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "    train_df['articleBody_Parsed_6'] = train_df['articleBody_Parsed_6'].str.replace(regex_stopword, '')\n",
        "category_codes = {\n",
        "    'discuss': 2,\n",
        "    'disagree': 1,\n",
        "    'agree': 0,\n",
        "    'unrelated': 3\n",
        "}\n",
        "train_df['Content_Parsed'] = train_df['articleBody_Parsed_6']\n",
        "train_df['Category_Code'] = train_df['Stance']\n",
        "train_df = train_df.replace({'Category_Code':category_codes})\n",
        "\n",
        "X_train = train_df['Content_Parsed']\n",
        "y_train = train_df['Category_Code']\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCJp6fVwKyb1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "2d762524-2478-4f20-f72d-9108f8a0b764"
      },
      "source": [
        "data = []\n",
        "train_df=[]\n",
        "trainbodies_df = []\n",
        "trainstances_df = []\n",
        "\n",
        "testbodies_url = 'https://raw.githubusercontent.com/StavroK/MtySaturdayAI2020/master/test_bodies.csv'\n",
        "testbodies_df = pd.read_csv(testbodies_url)\n",
        "teststances_url = 'https://raw.githubusercontent.com/StavroK/MtySaturdayAI2020/master/test_stances_unlabeled.csv'\n",
        "teststances_df = pd.read_csv(teststances_url)\n",
        "\n",
        "testbodies_df.sort_values(by='Body ID')\n",
        "teststances_df.sort_values(by='Body ID')\n",
        "\n",
        "\n",
        "test_df = pd.merge(testbodies_df,teststances_df, on=\"Body ID\")\n",
        "\n",
        "test_df['articleBody_Parsed_1'] = test_df['articleBody'].str.replace(\"\\r\", \" \")\n",
        "test_df['articleBody_Parsed_1'] = test_df['articleBody_Parsed_1'].str.replace(\"\\n\", \" \")\n",
        "test_df['articleBody_Parsed_1'] = test_df['articleBody_Parsed_1'].str.replace(\"    \", \" \")\n",
        "test_df['articleBody_Parsed_1'] = test_df['articleBody_Parsed_1'].str.replace('\"', '')\n",
        "test_df['articleBody_Parsed_2'] = test_df['articleBody_Parsed_1'].str.lower()\n",
        "punctuation_signs = list(\"?:!.,;\")\n",
        "test_df['articleBody_Parsed_3'] = test_df['articleBody_Parsed_2']\n",
        "\n",
        "for punct_sign in punctuation_signs:\n",
        "    test_df['articleBody_Parsed_3'] = test_df['articleBody_Parsed_3'].str.replace(punct_sign, '')\n",
        "\n",
        "test_df['articleBody_Parsed_4'] = test_df['articleBody_Parsed_3'].str.replace(\"'s\", \"\")\n",
        "nltk.download('punkt')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "nrows = len(train_df)\n",
        "lemmatized_text_list = []\n",
        "\n",
        "for row in range(0, nrows):\n",
        "    \n",
        "    lemmatized_list = []\n",
        "    text = test_df.loc[row]['articleBody_Parsed_4']\n",
        "    text_words = text.split(\" \")\n",
        "\n",
        "    for word in text_words:\n",
        "      lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
        "        \n",
        "    lemmatized_text = \" \".join(lemmatized_list)\n",
        "    lemmatized_text_list.append(lemmatized_text)\n",
        "test_df['articleBody_Parsed_5'] = lemmatized_text_list\n",
        "nltk.download('stopwords')\n",
        "stop_words = list(stopwords.words('english'))\n",
        "test_df['articleBody_Parsed_6'] = test_df['articleBody_Parsed_5']\n",
        "\n",
        "for stop_word in stop_words:\n",
        "\n",
        "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
        "    test_df['articleBody_Parsed_6'] = test_df['articleBody_Parsed_6'].str.replace(regex_stopword, '')\n",
        "category_codes = {\n",
        "    'discuss': 2,\n",
        "    'disagree': 1,\n",
        "    'agree': 0,\n",
        "    'unrelated': 3\n",
        "}\n",
        "test_df['Content_Parsed'] = test_df['articleBody_Parsed_6']\n",
        "test_df['Category_Code'] = test_df['Stance']\n",
        "test_df = train_df.replace({'Category_Code':category_codes})\n",
        "\n",
        "X_test = train_df['Content_Parsed']\n",
        "y_test = train_df['Category_Code']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-94cda9a2ff20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlemmatized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mlemmatized_text_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'articleBody_Parsed_5'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatized_text_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3636\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3638\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL_ry2glJyX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ngram_range = (1,2)\n",
        "min_df = 10\n",
        "max_df = 1.\n",
        "max_features = 300\n",
        "\n",
        "tfidf = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range=ngram_range,\n",
        "                        stop_words=None,\n",
        "                        lowercase=False,\n",
        "                        max_df=max_df,\n",
        "                        min_df=min_df,\n",
        "                        max_features=max_features,\n",
        "                        norm='l2',\n",
        "                        sublinear_tf=True)\n",
        "                        \n",
        "features_train = tfidf.fit_transform(X_train).toarray()\n",
        "labels_train = y_train\n",
        "print(features_train.shape)\n",
        "\n",
        "features_test = tfidf.transform(X_test).toarray()\n",
        "labels_test = y_test\n",
        "print(features_test.shape)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "for Product, category_id in sorted(category_codes.items()):\n",
        "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
        "    indices = np.argsort(features_chi2[0])\n",
        "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "    print(\"# '{}' category:\".format(Product))\n",
        "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
        "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
        "    print(\"\")\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__6Z1LIdKG6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X_train\n",
        "X_train.to_csv(\"/content/Standarized/X_train.csv\")\n",
        "    \n",
        "# X_test    \n",
        "X_test.to_csv(\"/content/Standarized/X_test.csv\")\n",
        "    \n",
        "# y_train\n",
        "y_train.to_csv(\"/content/Standarized/y_train.csv\")\n",
        "    \n",
        "# y_test\n",
        "y_test.to_csv(\"/content/Standarized/y_test.csv\")\n",
        "    \n",
        "# train_df\n",
        "train_df.to_csv(\"/content/Standarized/train_df.csv\")\n",
        "    \n",
        "# features_train\n",
        "np.savetxt('/content/Standarized/features_train.csv',features_train,delimiter=',')\n",
        "\n",
        "# labels_train\n",
        "np.savetxt('/content/Standarized/labels_train.csv',labels_train,delimiter=',')\n",
        "\n",
        "# features_test\n",
        "np.savetxt('/content/Standarized/features_test.csv',features_train,delimiter=',')\n",
        "\n",
        "# labels_test\n",
        "np.savetxt('/content/Standarized/labels_test.csv',labels_test,delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqWxur6xQSvK",
        "colab_type": "text"
      },
      "source": [
        "Let's check the dimension of our feature vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWzMMOrPQSvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(features_train.shape)\n",
        "print(features_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OrYcrkeQSvN",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation for Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO8w5xB5QSvO",
        "colab_type": "text"
      },
      "source": [
        "First, we can see what hyperparameters the model has:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWT4DbKNQSvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gb_0 = GradientBoostingClassifier(random_state = 8)\n",
        "\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(gb_0.get_params())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr2Vt6CaQSvS",
        "colab_type": "text"
      },
      "source": [
        "We'll tune the following ones:\n",
        "\n",
        "Tree-related hyperparameters:\n",
        "* `n_estimators` = number of trees in the forest.\n",
        "* `max_features` = max number of features considered for splitting a node\n",
        "* `max_depth` = max number of levels in each decision tree\n",
        "* `min_samples_split` = min number of data points placed in a node before the node is split\n",
        "* `min_samples_leaf` = min number of data points allowed in a leaf node\n",
        "\n",
        "Boosting-related hyperparameters:\n",
        "* `learning_rate`= learning rate shrinks the contribution of each tree by learning_rate.\n",
        "* `subsample`= the fraction of samples to be used for fitting the individual base learners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQAd1qF6QSvS",
        "colab_type": "text"
      },
      "source": [
        "### Randomized Search Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK2WJU7IQSvT",
        "colab_type": "text"
      },
      "source": [
        "We first need to define the grid. Since we have a huge amount of hyperparameters, we'll try few values for each one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x_uswewQSvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n_estimators\n",
        "n_estimators = [200, 800]\n",
        "\n",
        "# max_features\n",
        "max_features = ['auto', 'sqrt']\n",
        "\n",
        "# max_depth\n",
        "max_depth = [10, 40]\n",
        "max_depth.append(None)\n",
        "\n",
        "# min_samples_split\n",
        "min_samples_split = [10, 30, 50]\n",
        "\n",
        "# min_samples_leaf\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "# learning rate\n",
        "learning_rate = [.1, .5]\n",
        "\n",
        "# subsample\n",
        "subsample = [.5, 1.]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'learning_rate': learning_rate,\n",
        "               'subsample': subsample}\n",
        "\n",
        "pprint(random_grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL-5PgInQSvW",
        "colab_type": "text"
      },
      "source": [
        "Then, we'll perform the Random Search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu6nJ9PGQSvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First create the base model to tune\n",
        "gbc = GradientBoostingClassifier(random_state=8)\n",
        "\n",
        "# Definition of the random search\n",
        "random_search = RandomizedSearchCV(estimator=gbc,\n",
        "                                   param_distributions=random_grid,\n",
        "                                   n_iter=50,\n",
        "                                   scoring='accuracy',\n",
        "                                   cv=3, \n",
        "                                   verbose=1, \n",
        "                                   random_state=8)\n",
        "\n",
        "# Fit the random search model\n",
        "random_search.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOS16ye5QSvZ",
        "colab_type": "text"
      },
      "source": [
        "We can see the best hyperparameters resulting from the Random Search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT8yOPndQSvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The best hyperparameters from Random Search are:\")\n",
        "print(random_search.best_params_)\n",
        "print(\"\")\n",
        "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
        "print(random_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRGdqr4BQSvb",
        "colab_type": "text"
      },
      "source": [
        "After that, we can do a more exhaustive search centered in those values:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHhrpP5uQSvc",
        "colab_type": "text"
      },
      "source": [
        "### Grid Search Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVySMnq0QSvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the parameter grid based on the results of random search \n",
        "max_depth = [5, 10, 15]\n",
        "max_features = ['sqrt']\n",
        "min_samples_leaf = [2]\n",
        "min_samples_split = [50, 100]\n",
        "n_estimators = [800]\n",
        "learning_rate = [.1, .5]\n",
        "subsample = [1.]\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': max_depth,\n",
        "    'max_features': max_features,\n",
        "    'min_samples_leaf': min_samples_leaf,\n",
        "    'min_samples_split': min_samples_split,\n",
        "    'n_estimators': n_estimators,\n",
        "    'learning_rate': learning_rate,\n",
        "    'subsample': subsample\n",
        "\n",
        "}\n",
        "\n",
        "# Create a base model\n",
        "gbc = GradientBoostingClassifier(random_state=8)\n",
        "\n",
        "# Manually create the splits in CV in order to be able to fix a random_state (GridSearchCV doesn't have that argument)\n",
        "cv_sets = ShuffleSplit(n_splits = 3, test_size = .33, random_state = 8)\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator=gbc, \n",
        "                           param_grid=param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=cv_sets,\n",
        "                           verbose=1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqhCGQzYQSve",
        "colab_type": "text"
      },
      "source": [
        "The best hyperparameters turn out to be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDIYmvNnQSve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The best hyperparameters from Grid Search are:\")\n",
        "print(grid_search.best_params_)\n",
        "print(\"\")\n",
        "print(\"The mean accuracy of a model with these hyperparameters is:\")\n",
        "print(grid_search.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQRXCXL1QSvh",
        "colab_type": "text"
      },
      "source": [
        "Let's save the model in `best_gbc`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Uwu1cHQSvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_gbc = grid_search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4yscAKMQSvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_gbc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNUY5WhWQSvl",
        "colab_type": "text"
      },
      "source": [
        "We now know the best gradient boosting model. Let's fit it and see how it performs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7H_s9H_QSvm",
        "colab_type": "text"
      },
      "source": [
        "## Model fit and performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw9LYDtDQSvm",
        "colab_type": "text"
      },
      "source": [
        "Now, we can fit the model to our training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFO18qEKQSvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_gbc.fit(features_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81x94XNnQSvo",
        "colab_type": "text"
      },
      "source": [
        "And get the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUvac0_1QSvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gbc_pred = best_gbc.predict(features_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTRPGoX_QSvr",
        "colab_type": "text"
      },
      "source": [
        "The conditional class probabilities can be obtained by typing:\n",
        "\n",
        "`gbc_pred = best_gbc.predict_proba(features_test)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVLxE-pwQSvr",
        "colab_type": "text"
      },
      "source": [
        "For performance analysis, we will use the confusion matrix, the classification report and the accuracy on both training and test data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOQBfXIiQSvs",
        "colab_type": "text"
      },
      "source": [
        "#### Training accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0QOJ1gxQSvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training accuracy\n",
        "print(\"The training accuracy is: \")\n",
        "print(accuracy_score(labels_train, best_gbc.predict(features_train)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrzDjSuIQSvu",
        "colab_type": "text"
      },
      "source": [
        "#### Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0ELIv42QSvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test accuracy\n",
        "print(\"The test accuracy is: \")\n",
        "print(accuracy_score(labels_test, gbc_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OlQevK_QSvw",
        "colab_type": "text"
      },
      "source": [
        "#### Classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIFYrnM7QSvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classification report\n",
        "print(\"Classification report\")\n",
        "print(classification_report(labels_test,gbc_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx2wnQZ9QSvy",
        "colab_type": "text"
      },
      "source": [
        "#### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDNc50WFQSvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aux_df = df[['Category', 'Category_Code']].drop_duplicates().sort_values('Category_Code')\n",
        "conf_matrix = confusion_matrix(labels_test, gbc_pred)\n",
        "plt.figure(figsize=(12.8,6))\n",
        "sns.heatmap(conf_matrix, \n",
        "            annot=True,\n",
        "            xticklabels=aux_df['Category'].values, \n",
        "            yticklabels=aux_df['Category'].values,\n",
        "            cmap=\"Blues\")\n",
        "plt.ylabel('Predicted')\n",
        "plt.xlabel('Actual')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGKfe_SQQSv0",
        "colab_type": "text"
      },
      "source": [
        "At this point, we could get the average time the model takes to get predictions. We want the algorithm to be fast since we are creating an app which will gather data from the internet and get the predicted categories. However, since the difference when predicting 10-20 observations will be very little, we won't take this into account.\n",
        "\n",
        "However, the code below could do this task:\n",
        "\n",
        "```python\n",
        "features_time = features_train\n",
        "elapsed_list = []\n",
        "for i in range(0,10):\n",
        "    \n",
        "    start = time.time()\n",
        "    predictions = best_lrc.predict(features_time)\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    elapsed_list.append(elapsed)\n",
        "\n",
        "mean_time_elapsed = np.mean(elapsed_list)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekhCfPIQQSv0",
        "colab_type": "text"
      },
      "source": [
        "Let's see if the hyperparameter tuning process has returned a better model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1w_DYQmQSv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = GradientBoostingClassifier(random_state = 8)\n",
        "base_model.fit(features_train, labels_train)\n",
        "accuracy_score(labels_test, base_model.predict(features_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kiIyoCBQSv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_gbc.fit(features_train, labels_train)\n",
        "accuracy_score(labels_test, best_gbc.predict(features_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdvDAVwQQSv5",
        "colab_type": "text"
      },
      "source": [
        "We'll create a dataset with a model summary to compare models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76cW4-JYQSv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {\n",
        "     'Model': 'Gradient Boosting',\n",
        "     'Training Set Accuracy': accuracy_score(labels_train, best_gbc.predict(features_train)),\n",
        "     'Test Set Accuracy': accuracy_score(labels_test, gbc_pred)\n",
        "}\n",
        "\n",
        "df_models_gbc = pd.DataFrame(d, index=[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t71AjtWQSv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_models_gbc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajx_YRAdQSv8",
        "colab_type": "text"
      },
      "source": [
        "Let's save the model and this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQUBvzvtQSv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('Models/best_gbc.pickle', 'wb') as output:\n",
        "    pickle.dump(best_gbc, output)\n",
        "    \n",
        "with open('Models/df_models_gbc.pickle', 'wb') as output:\n",
        "    pickle.dump(df_models_gbc, output)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature engineering.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNh35LcwjbDkut8fztPuXLd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StavroK/MtySaturdayAI2020/blob/master/Feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ZntwLOvMfk",
        "colab_type": "code",
        "outputId": "63e6f06d-9d5f-4f0e-80f3-e35cbee03982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "%cd /usr/local/lib/python3.6/dist-packages\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import re\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import chi2\n",
        "import warnings\n",
        "from csv import DictReader\n",
        "from csv import DictWriter\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from util import *\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxX2qY1Fc1Lc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_train_instances = \"train_stances.csv\"\n",
        "file_train_bodies = \"train_bodies.csv\"\n",
        "file_test_instances = \"test_stances_unlabeled.csv\"\n",
        "file_test_bodies = \"test_bodies.csv\"\n",
        "file_predictions = 'predictions_test.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYkPVgciEqOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_ref = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}\n",
        "label_ref_rev = {0: 'agree', 1: 'disagree', 2: 'discuss', 3: 'unrelated'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkvCLb63FZcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = list(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyeRAdfPGWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ModelData:\n",
        "\n",
        "    def __init__(self, file_instances, file_bodies):\n",
        "\n",
        "        # Load data\n",
        "        self.instances = self.read(file_instances)\n",
        "        bodies = self.read(file_bodies)\n",
        "        self.heads = {}\n",
        "        self.bodies = {}\n",
        "\n",
        "        # Process instances\n",
        "        for instance in self.instances:\n",
        "            if instance['Headline'] not in self.heads:\n",
        "                head_id = len(self.heads)\n",
        "                self.heads[instance['Headline']] = head_id\n",
        "            instance['Body ID'] = int(instance['Body ID'])\n",
        "\n",
        "        # Process bodies\n",
        "        for body in bodies:\n",
        "            self.bodies[int(body['Body ID'])] = body['articleBody']\n",
        "\n",
        "    def read(self, filename):\n",
        "\n",
        "        # Initialise\n",
        "        rows = []\n",
        "\n",
        "        # Process file\n",
        "        with open(filename, \"r\", encoding='utf-8') as table:\n",
        "            r = DictReader(table)\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "\n",
        "        return rows\n",
        "\n",
        "\n",
        "# Define relevant functions\n",
        "def pipeline_train(train, test, lim_unigram):\n",
        "\n",
        "    # Initialise\n",
        "    heads = []\n",
        "    heads_track = {}\n",
        "    bodies = []\n",
        "    bodies_track = {}\n",
        "    body_ids = []\n",
        "    id_ref = {}\n",
        "    train_set = []\n",
        "    train_stances = []\n",
        "    cos_track = {}\n",
        "    test_heads = []\n",
        "    test_heads_track = {}\n",
        "    test_bodies = []\n",
        "    test_bodies_track = {}\n",
        "    test_body_ids = []\n",
        "    head_tfidf_track = {}\n",
        "    body_tfidf_track = {}\n",
        "\n",
        "    # Identify unique heads and bodies\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            heads.append(head)\n",
        "            heads_track[head] = 1\n",
        "        if body_id not in bodies_track:\n",
        "            bodies.append(train.bodies[body_id])\n",
        "            bodies_track[body_id] = 1\n",
        "            body_ids.append(body_id)\n",
        "\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in test_heads_track:\n",
        "            test_heads.append(head)\n",
        "            test_heads_track[head] = 1\n",
        "        if body_id not in test_bodies_track:\n",
        "            test_bodies.append(test.bodies[body_id])\n",
        "            test_bodies_track[body_id] = 1\n",
        "            test_body_ids.append(body_id)\n",
        "\n",
        "    # Create reference dictionary\n",
        "    for i, elem in enumerate(heads + body_ids):\n",
        "        id_ref[elem] = i\n",
        "\n",
        "    # Create vectorizers and BOW and TF arrays for train set\n",
        "    bow_vectorizer = CountVectorizer(max_features=lim_unigram, stop_words=stop_words)\n",
        "    bow = bow_vectorizer.fit_transform(heads + bodies)  # Train set only\n",
        "\n",
        "    tfreq_vectorizer = TfidfTransformer(use_idf=False).fit(bow)\n",
        "    tfreq = tfreq_vectorizer.transform(bow).toarray()  # Train set only\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=lim_unigram, stop_words=stop_words).\\\n",
        "        fit(heads + bodies + test_heads + test_bodies)  # Train and test sets\n",
        "\n",
        "    # Process train set\n",
        "    for instance in train.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        head_tf = tfreq[id_ref[head]].reshape(1, -1)\n",
        "        body_tf = tfreq[id_ref[body_id]].reshape(1, -1)\n",
        "        if head not in head_tfidf_track:\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray()\n",
        "            head_tfidf_track[head] = head_tfidf\n",
        "        else:\n",
        "            head_tfidf = head_tfidf_track[head]\n",
        "        if body_id not in body_tfidf_track:\n",
        "            body_tfidf = tfidf_vectorizer.transform([train.bodies[body_id]]).toarray()\n",
        "            body_tfidf_track[body_id] = body_tfidf\n",
        "        else:\n",
        "            body_tfidf = body_tfidf_track[body_id]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        train_set.append(feat_vec)\n",
        "        train_stances.append(label_ref[instance['Stance']])\n",
        "\n",
        "    return train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer\n",
        "\n",
        "\n",
        "def pipeline_test(test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer):\n",
        "\n",
        "    # Initialise\n",
        "    test_set = []\n",
        "    heads_track = {}\n",
        "    bodies_track = {}\n",
        "    cos_track = {}\n",
        "\n",
        "    # Process test set\n",
        "    for instance in test.instances:\n",
        "        head = instance['Headline']\n",
        "        body_id = instance['Body ID']\n",
        "        if head not in heads_track:\n",
        "            head_bow = bow_vectorizer.transform([head]).toarray()\n",
        "            head_tf = tfreq_vectorizer.transform(head_bow).toarray()[0].reshape(1, -1)\n",
        "            head_tfidf = tfidf_vectorizer.transform([head]).toarray().reshape(1, -1)\n",
        "            heads_track[head] = (head_tf, head_tfidf)\n",
        "        else:\n",
        "            head_tf = heads_track[head][0]\n",
        "            head_tfidf = heads_track[head][1]\n",
        "        if body_id not in bodies_track:\n",
        "            body_bow = bow_vectorizer.transform([test.bodies[body_id]]).toarray()\n",
        "            body_tf = tfreq_vectorizer.transform(body_bow).toarray()[0].reshape(1, -1)\n",
        "            body_tfidf = tfidf_vectorizer.transform([test.bodies[body_id]]).toarray().reshape(1, -1)\n",
        "            bodies_track[body_id] = (body_tf, body_tfidf)\n",
        "        else:\n",
        "            body_tf = bodies_track[body_id][0]\n",
        "            body_tfidf = bodies_track[body_id][1]\n",
        "        if (head, body_id) not in cos_track:\n",
        "            tfidf_cos = cosine_similarity(head_tfidf, body_tfidf)[0].reshape(1, 1)\n",
        "            cos_track[(head, body_id)] = tfidf_cos\n",
        "        else:\n",
        "            tfidf_cos = cos_track[(head, body_id)]\n",
        "        feat_vec = np.squeeze(np.c_[head_tf, body_tf, tfidf_cos])\n",
        "        test_set.append(feat_vec)\n",
        "\n",
        "    return test_set\n",
        "\n",
        "\n",
        "def load_model(sess):\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, './model/model.checkpoint')\n",
        "\n",
        "\n",
        "def save_predictions(pred, file):\n",
        "\n",
        "    with open(file, 'w') as csvfile:\n",
        "        fieldnames = ['Stance']\n",
        "        writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for instance in pred:\n",
        "            writer.writerow({'Stance': label_ref_rev[instance]})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}